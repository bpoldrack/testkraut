Source: testkraut
Section: python
Priority: extra
Maintainer: NeuroDebian Team <team@neuro.debian.net>
Uploaders: Michael Hanke <mih@debian.org>, Yaroslav Halchenko <debian@onerussian.com>
Build-Depends: debhelper (>= 7.2.18), python-all (>= 2.6),
 python (>= 2.7) | python-argparse, python-support (>= 0.6), python-numpy,
 help2man, python-sphinx, python-nose, python-nibabel, python-scipy, strace
Standards-Version: 3.9.3
Homepage: https://github.com/neurodebian/testkraut
Vcs-Browser: http://git.debian.org/?p=pkg-exppsy/testkraut.git
Vcs-Git: git://git.debian.org/git/pkg-exppsy/testkraut.git
XS-Python-Version: >= 2.6

Package: testkraut
Architecture: all
Depends: ${misc:Depends}, ${python:Depends}, python-numpy
 libjs-underscore, libjs-jquery, python (>= 2.7) | python-argparse,
Recommends: strace, python-scipy, python-colorama, python-apt
Provides: ${python:Provides}
XB-Python-Version: ${python:Versions}
Description: test and evaluate heterogeneous data processing pipelines
 This is a framework for software testing. That being said, testkraut tries to
 minimize the overlap with the scopes of unit testing, regression testing, and
 continuous integration testing. Instead, it aims to complement these kinds of
 testing, and is able to re-use them, or can be integrated with them.
 .
 In a nutshell testkraut helps to facilitate statistical analysis of test
 results. In particular, it focuses on two main scenarios:
 .
  * Comparing results of a single (test) implementation across different or
    changing computational environments (think: different operating systems,
    different hardware, or the same machine before an after a software upgrade).
  * Comparing results of different (test) implementations generating similar
    output from identical input (think: performance of various signal detection
    algorithms).
 .
 While such things can be done using other available tools as well, testkraut
 aims to provide a lightweight, yet comprehensive description of a test run.
 Such a description allows for decoupling test result generation and analysis
 – opening up the opportunity to “crowd-source” software testing efforts, and
 aggregate results beyond the scope of a single project, lab, company, or site.
